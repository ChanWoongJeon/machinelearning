{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "소프트웨어학부 20186663 전찬웅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jeon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jeon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.datasets import load_files\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "review_data = load_files(r\"movie_review\")\n",
    "X, y = review_data.data, review_data.target\n",
    "\n",
    "documents = []\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X = tfidfconverter.fit_transform(X).toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
    "y_train = y_train.reshape((1401,1))\n",
    "y_test = y_test.reshape((601,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1401, 1500)\n",
      "(601, 1500)\n",
      "(1401, 1)\n",
      "(601, 1)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(y_train))\n",
    "print(np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta1 = np.random.randn(1500,256)\n",
    "theta2 = np.random.randn(256,64)\n",
    "theta3 = np.random.randn(64,1)\n",
    "bias1 = np.random.randn(1,256)\n",
    "bias2 = np.random.randn(1,64)\n",
    "bias3 = np.random.randn(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "cost_ = []\n",
    "cost_test_ = []\n",
    "accuracy = []\n",
    "test_accuracy = []\n",
    "lambd = 2\n",
    "for i in range(10000):\n",
    "    a=0\n",
    "    b=0\n",
    "    Z1 = np.dot(X_train,theta1)+bias1\n",
    "    A1 = 1/(1+np.exp(-Z1))\n",
    "    Z2 = np.dot(A1,theta2)+bias2\n",
    "    A2 = 1/(1+np.exp(-Z2))\n",
    "    Z3 = np.dot(A2,theta3)+bias3\n",
    "    A3 = 1/(1+np.exp(-Z3))\n",
    "    Z1_test = np.dot(X_test,theta1)\n",
    "    A1_test = 1/(1+np.exp(-Z1_test))\n",
    "    Z2_test = np.dot(A1_test,theta2)\n",
    "    A2_test = 1/(1+np.exp(-Z2_test))\n",
    "    Z3_test = np.dot(A2_test,theta3)\n",
    "    A3_test = 1/(1+np.exp(-Z3_test))\n",
    "    cost = (-y_train*np.log(A3)-(1-y_train)*np.log(1-A3)).sum()/len(X_train)+(np.sum(np.square(theta1))+np.sum(np.square(theta2))+np.sum(np.square(theta3)))*lambd/2/len(X_train)\n",
    "    cost_.append(cost)\n",
    "    cost_test = (-y_test*np.log(A3_test)-(1-y_test)*np.log(1-A3_test)).sum()/len(X_test)\n",
    "    cost_test_.append(cost_test)\n",
    "    for j in range(len(X_train)):\n",
    "        if np.round(A3[j])==y_train[j]:\n",
    "            a+=1.0\n",
    "    for j in range(len(X_test)):\n",
    "        if np.round(A3_test[j])==y_test[j]:\n",
    "            b+=1.0\n",
    "    accuracy.append(a*100/len(X_train))\n",
    "    test_accuracy.append(b*100/len(X_test))\n",
    "    dL_dZ3 = A3-y_train                               #(m,1)\n",
    "    dZ3_dtheta3 = A2                                  #(m,64)\n",
    "    dZ3_dbias3 = 1                                    #(1,1)\n",
    "    dZ3_dA2 = theta3                                  #(64,1)\n",
    "    dA2_dZ2 = 1/(1+np.exp(-Z2))*(1-1/(1+np.exp(-Z2))) #(m,64)\n",
    "    dZ2_dtheta2 = A1                                  #(m,256)\n",
    "    dZ2_dbias2 = 1                                    #(1,1)\n",
    "    dZ2_dA1 = theta2                                  #(256,64)\n",
    "    dA1_dZ1 = 1/(1+np.exp(-Z1))*(1-1/(1+np.exp(-Z1))) #(m,256)\n",
    "    dZ1_dtheta1 = X_train                             #(m,784)\n",
    "    dZ1_dbias1 = 1                                    #(1,1)\n",
    "    dtheta3 = np.dot(dZ3_dtheta3.T,dL_dZ3)+theta3*lambd/len(X_train)\n",
    "    dbias3 = dL_dZ3\n",
    "    dtheta2 = np.dot(dZ2_dtheta2.T, dA2_dZ2*np.dot(dL_dZ3,dZ3_dA2.T))+theta2*lambd/len(X_train)\n",
    "    dbias2 = dA2_dZ2*np.dot(dL_dZ3,dZ3_dA2.T)\n",
    "    dtheta1 = np.dot(dZ1_dtheta1.T, dA1_dZ1*np.dot(dL_dZ3, np.dot(dZ3_dA2.T,dZ2_dA1.T)))+theta1*lambd/len(X_train)\n",
    "    dbias1 = dA1_dZ1*np.dot(dL_dZ3, np.dot(dZ3_dA2.T,dZ2_dA1.T))\n",
    "    theta3 = theta3-learning_rate*dtheta3/len(X_train)\n",
    "    bias3 = bias3-learning_rate*dbias3/len(X_train)\n",
    "    theta2 = theta2-learning_rate*dtheta2/len(X_train)\n",
    "    bias2 = bias2-learning_rate*dbias2/len(X_train)\n",
    "    theta1 = theta1-learning_rate*dtheta1/len(X_train)\n",
    "    bias1 = bias1-learning_rate*dbias1/len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title(\"assignment10-1(cost)\")\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.plot(range(10000),cost_,color='b')\n",
    "plt.plot(range(10000),cost_test_,color='r')\n",
    "plt.legend(['train cost','test cost'], prop={'size': 20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.title(\"assignment10-2(accuracy)\")\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.plot(range(10000),accuracy,color='b')\n",
    "plt.plot(range(10000),test_accuracy,color='r')\n",
    "plt.legend(['train accuracy','test accuracy'], prop={'size': 20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
